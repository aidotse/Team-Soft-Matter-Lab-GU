{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1B - Training of virtual staining of brightfield images (60x)\n",
    "\n",
    "Example code to train a neural network to virtually stain brightfield images captured with the 60x magnification objective obtaining the corresponding images for nuclei, lipids and cytoplasm.\n",
    "\n",
    "version 1.0 <br />\n",
    "15 November 2020 <br />\n",
    "Benjamin Midtvedt, Jes√∫s Pineda Castro, Saga Helgadottir, Daniel Midtvedt & Giovanni Volpe <br />\n",
    "Soft Matter Lab @ GU <br />\n",
    "http://www.softmatterlab.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    " \n",
    "Import all necessary packages. These include standard Python packages as well as the core of DeepTrack 2.0 (`deeptrack`) and some specialized classes for this virtual staining (`apido`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# DeepTrack 2.0 code\n",
    "import apido\n",
    "from apido import deeptrack as dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define input and output\n",
    "\n",
    "Set constants to determine the input and output images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 User-defined constants for loading data and saving model\n",
    "\n",
    "Constants defined by the user:\n",
    "\n",
    "* `DATASET_PATH`: Input path (not including the magnification folder)\n",
    "\n",
    "* `OUTPUT_PATH`: Output path (not including the magnication folder)\n",
    "\n",
    "* `WELLS`: Name of the wells on which to predict\n",
    "\n",
    "* `SITES`: \"all\" or list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION = \"20x\"\n",
    "DATASET_PATH = \"../astra_data_readonly/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inferred constants\n",
    "\n",
    "Constants inferred from the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION = \"60x\"\n",
    "file_name_struct = \"AssayPlate_Greiner_#655090_{0}_T0001F{1}L01A0{2}Z0{3}C0{2}.tif\"\n",
    "\n",
    "PATH_TO_OUTPUT = os.path.normpath(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer full path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "_glob_struct = os.path.join(DATASET_PATH, MAGNIFICATION + \"*/\")\n",
    "_glob_results = glob.glob(_glob_struct)\n",
    "\n",
    "if len(_glob_results) == 0:\n",
    "    raise ValueError(\"No path found matching glob {0}\".format(_glob_struct))\n",
    "elif len(_glob_results) > 1:\n",
    "    from warnings import warn\n",
    "    warn(\"Multiple paths found! Using {0}\".format(_glob_results[0]))\n",
    "\n",
    "PATH_TO_MAGNIFICATION = os.path.normpath(_glob_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: \t test_data\\60x images\n",
      "Saving results to: \t models\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images from: \\t\", PATH_TO_MAGNIFICATION)\n",
    "print(\"Saving results to: \\t\", PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load train data\n",
    "\n",
    "We define a data pipeline for loading images from storage. This uses DeepTrack 2.0, and follows the structure of\n",
    "\n",
    "1. Load each z-slice of a well-site combination and concatenate them.\n",
    "2. Pad the volume such that the first two dimensions are multiples of 32 (required by the model).\n",
    "3. Correct for misalignment of the fluorescence channel and the brightfield channel (by a pre-calculated parametrization of the offset as a function of magnification and the site)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find all wells and sites\n",
    "\n",
    "We create an iterator over each well and site. `Itertools.product` produces an iterator over each combination of its input. In this case, each site in each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = glob.glob(os.path.join(DATASET_PATH, \"*C01.tif\"))\n",
    "\n",
    "SITES = [re.findall(\"F([0-9]{3})\", f)[-1] for f in file_list]\n",
    "WELLS = [re.findall(\"_([A-Z][0-9]{2})_\", f)[-1] for f in file_list]\n",
    "\n",
    "wells_and_sites = list(zip(WELLS, SITES))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The root feature\n",
    "\n",
    "We use DeepTrack 2.0 to define the data loader pipeline. The pipeline is a sequence of `features`, which perform computations, controlled by `properties`, which are defined when creating the features. (Note that we any property with any name and value to a feature; if a property is not used by the feature, we refer to it as a dummy property.)\n",
    "\n",
    "The feature `root` is a `DummyFeature`, which is just a container of dummy properties and does not perform any computations.\n",
    "It takes the following arguments:\n",
    "\n",
    "* `well_site_tuple` is a dummy property that cycles through the well-site combinations in `wells_and_sites`\n",
    "* `well` is a dummy property that extracts the well from the `well_site_tuple`\n",
    "* `site` is a dummy property that extracts the site from the `well_site_tuple`\n",
    "\n",
    "Note that `well` and `site` are functions that take `well_site_tuple` as argument. These are dependent properties, and DeepTrack 2.0 will automatically ensure that they receive the correct input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = dt.DummyFeature(\n",
    "    well_site_tuple=itertools.cycle(wells_and_sites), # On each update, root will grab the next value from this iterator\n",
    "    well=lambda well_site_tuple: well_site_tuple[0],  # Grabs the well from the well_site_tuple\n",
    "    site=lambda well_site_tuple: well_site_tuple[1],  # Grabs the site from the well_site_tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The brightfield image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a brightfield stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightfield_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_name=lambda well, site: file_name_struct.format(well, site, 4, 1),\n",
    "    path=lambda file_name: os.path.join(PATH_TO_MAGNIFICATION, file_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The fluorescence image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a fluorescence stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_name=lambda well, site: file_name_struct.format(well, site, 2, 1),\n",
    "    path=lambda file_name: os.path.join(PATH_TO_MAGNIFICATION, file_name),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrected_brightfield = brightfield_loader + correct_offset\n",
    "\n",
    "data_pair = dt.Combine([corrected_brightfield, fluorescence_loader]) + dt.AsType(\"float64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define generator\n",
    "\n",
    "We use generators to interface DeepTrack 2.0 features with Keras training routines. In DeepTrack 2.0, we have defined some special generators that speed up training. Here, we will use `deeptrack.ContinuousGenerator`, which continuosly geenrate augmented training images and makes them available for training the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, a, b, c, d):\n",
    "    x = X[0]\n",
    "    y = X[1]\n",
    "    return a * np.exp(-((x - b) ** 2 + (y - c) ** 2) / (2 * d ** 2))\n",
    "\n",
    "def calculate_total_error(num_files):\n",
    "    all_out = []\n",
    "    \n",
    "    dat = np.zeros((4, 4))\n",
    "    dbt = np.zeros((4, 4))\n",
    "    for iii in range(num_files):\n",
    "        da = np.zeros((4, 4))\n",
    "        db = np.zeros((4, 4))\n",
    "        im_1, im_2 = data_pair.update().resolve()\n",
    "        \n",
    "        patch_size=512\n",
    "        res = []\n",
    "        countx = 1\n",
    "        _x = np.arange(-patch_size / 2, patch_size / 2)\n",
    "\n",
    "        X, Y = np.meshgrid(_x, _x)\n",
    "        Xsmall = X[\n",
    "            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "        ]\n",
    "        Ysmall = Y[\n",
    "            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        for i in range(0, im_1.shape[0] - patch_size, patch_size):\n",
    "            county = 0\n",
    "            for j in range(0, im_1.shape[1] - patch_size, patch_size):\n",
    "                for z in range(1):\n",
    "                    #     county=1;\n",
    "                    #     for j=1:patch_size:size(image_1,2)-patch_size\n",
    "\n",
    "                    \n",
    "                    corr = np.fft.fftshift(\n",
    "                        np.fft.ifft2(\n",
    "                            np.fft.fft2(im_1[i : i + patch_size, j : j + patch_size, 0])\n",
    "                            * np.conjugate(\n",
    "                                np.fft.fft2(\n",
    "                                    im_2[i : i + patch_size, j : j + patch_size, 1]\n",
    "                                )\n",
    "                            )\n",
    "                        )\n",
    "                    )\n",
    "    \n",
    "\n",
    "                    corrsmall = np.abs(\n",
    "                        corr[\n",
    "                            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "                            patch_size // 2 - 10 : patch_size // 2 + 11,\n",
    "                        ]\n",
    "                    )\n",
    "\n",
    "\n",
    "                    m = np.max(corrsmall)\n",
    "                    try:\n",
    "                        opt, p = optimize.curve_fit(\n",
    "                            fit,\n",
    "                            np.array([Xsmall.flatten(), Ysmall.flatten()]),\n",
    "                            corrsmall.flatten(),\n",
    "                            [m, 0, 0, 5],\n",
    "                        )\n",
    "\n",
    "                        a = opt[1]\n",
    "                        b = opt[2]\n",
    "                        res.append((a, b))\n",
    "\n",
    "                    except Exception as e:\n",
    "#                         print(e)\n",
    "                        pass\n",
    "        all_out.append((np.median(res, axis=0))\n",
    "    \n",
    "    return all_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patience = 10\n",
    "Bx = Ax = x = By = Ay = y = 0\n",
    "\n",
    "results = []\n",
    "scores = []\n",
    "\n",
    "scale=1\n",
    "upper = 1.002\n",
    "lower = 0.998\n",
    "\n",
    "step_length = 0.0002\n",
    "\n",
    "while len(results) == 0 or len(scores) - np.argmin(scores) < patience:\n",
    "    \n",
    "\n",
    "    \n",
    "    X, A, B = offset(48, \n",
    "        Bx= Bx,\n",
    "        Ax= Ax,\n",
    "        x=  x,\n",
    "        By= By,\n",
    "        Ay= Ay,\n",
    "        y=  y,\n",
    "        scale=scale)\n",
    "    \n",
    "    mDA = np.mean(A[:4, :3] - A[:4, 1:4])\n",
    "    mDB = np.mean(B[:3, :4] - B[1:4, :4])\n",
    "    print(scale, mDA, mDB)\n",
    "    \n",
    "    if mDA + mDB < 0:\n",
    "        scale -= step_length\n",
    "        \n",
    "    else:\n",
    "        scale += step_length\n",
    "        \n",
    "        \n",
    "    step_length = step_length * 0.95\n",
    "    \n",
    "    x2, y2 = list(zip(*X))\n",
    "    \n",
    "    \n",
    "    ang = np.linspace(0, 2 * np.pi / 6 * len(x2), len(x2))\n",
    "    def func(x, a, b, c):\n",
    "        return a * np.cos(x) + b * np.sin(x) + c\n",
    "\n",
    "    score = np.sum(np.abs(X))\n",
    "    \n",
    "    scores.append(score)\n",
    "    results.append((Bx, Ax, x, By, Ay, y))\n",
    "    \n",
    "    x2n = func(ang, Bx, Ax, x) + x2\n",
    "    y2n = func(ang, By, Ay, y) + y2\n",
    "    \n",
    "    Bx, Ax, x = optimize.curve_fit(func, ang, x2n, [0, 0, 0])[0]\n",
    "    By, Ay, y = optimize.curve_fit(func, ang, y2n, [0, 0, 0])[0]\n",
    "    \n",
    "    plt.plot(X)\n",
    "    plt.plot(func(ang, Bx, Ax, x), linestyle=\":\")\n",
    "    plt.plot(func(ang, By, Ay, y), linestyle=\":\")\n",
    "    plt.show()\n",
    "\n",
    "    print(score, len(scores) - np.argmin(scores))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
