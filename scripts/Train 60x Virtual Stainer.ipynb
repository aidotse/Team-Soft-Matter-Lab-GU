{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1B - Training of virtual staining of brightfield images (60x)\n",
    "\n",
    "Example code to train a neural network to virtually stain brightfield images captured with the 60x magnification objective obtaining the corresponding images for nuclei, lipids and cytoplasm.\n",
    "\n",
    "version 1.0 <br />\n",
    "15 November 2020 <br />\n",
    "Benjamin Midtvedt, Jes√∫s Pineda Castro, Saga Helgadottir, Daniel Midtvedt & Giovanni Volpe <br />\n",
    "Soft Matter Lab @ GU <br />\n",
    "http://www.softmatterlab.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Imports\n",
    " \n",
    "Import all necessary packages. These include standard Python packages as well as the core of DeepTrack 2.0 (`deeptrack`) and some specialized classes for this virtual staining (`apido`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# DeepTrack 2.0 code\n",
    "import deeptrack as dt\n",
    "import apido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Define input and output\n",
    "\n",
    "Set constants to determine the input and output images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Neural-network model parameters\n",
    "\n",
    "Parameters of the neural network model. These are:\n",
    "\n",
    "* `GENERATOR_BREADTH`: determines the width of the input image as `GENERATOR_BREADTH * 32` (e.g., `GENERATOR_BREADTH = 32` corresponds to an input image size `532`)\n",
    "\n",
    "* `GENERATOR_DEPTH`: Depth of the generator U-Net\n",
    "\n",
    "* `DISCRIMINATOR_DEPTH`: Depth of the discriminator convolutional encoder\n",
    "\n",
    "* `MAE_LOSS_WEIGHT`: the weighting of the MAE loss vs. the adversarial loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BREADTH = 16\n",
    "GENERATOR_DEPTH = 5\n",
    "DISCRIMINATOR_DEPTH = 5\n",
    "MAE_LOSS_WEIGHT = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 User-defined constants for loading data and saving model\n",
    "\n",
    "Constants defined by the user:\n",
    "\n",
    "* `DATASET_PATH`: Input path (not including the magnification folder)\n",
    "\n",
    "* `OUTPUT_PATH`: Output path (not including the magnication folder)\n",
    "\n",
    "* `WELLS`: Name of the wells on which to predict\n",
    "\n",
    "* `SITES`: \"all\" or list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"./train_data/\" \n",
    "OUTPUT_PATH = \"./models/\"\n",
    "WELLS = [\"C02\"]\n",
    "SITES = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inferred constants\n",
    "\n",
    "Constants inferred from the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION = \"60x\"\n",
    "file_name_struct = \"AssayPlate_Greiner_#655090_{0}_T0001F{1}L01A0{2}Z0{3}C0{2}.tif\"\n",
    "\n",
    "PATH_TO_OUTPUT = os.path.normpath(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer full path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_glob_struct = os.path.join(DATASET_PATH, MAGNIFICATION + \"*/\")\n",
    "_glob_results = glob.glob(_glob_struct)\n",
    "\n",
    "if len(_glob_results) == 0:\n",
    "    raise ValueError(\"No path found matching glob {0}\".format(_glob_struct))\n",
    "elif len(_glob_results) > 1:\n",
    "    from warnings import warn\n",
    "    warn(\"Multiple paths found! Using {0}\".format(_glob_results[0]))\n",
    "\n",
    "PATH_TO_MAGNIFICATION = os.path.normpath(_glob_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: \t local_data\\60x images\n",
      "Saving results to: \t models\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images from: \\t\", PATH_TO_MAGNIFICATION)\n",
    "print(\"Saving results to: \\t\", PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load train data\n",
    "\n",
    "We define a data pipeline for loading images from storage. This uses DeepTrack 2.0, and follows the structure of\n",
    "\n",
    "1. Load each z-slice of a well-site combination and concatenate them.\n",
    "2. Pad the volume such that the first two dimensions are multiples of 32 (required by the model).\n",
    "3. Correct for misalignment of the fluorescence channel and the brightfield channel (by a pre-calculated parametrization of the offset as a function of magnification and the site)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Find all wells and sites\n",
    "\n",
    "We create an iterator over each well and site. `Itertools.product` produces an iterator over each combination of its input. In this case, each site in each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_and_sites = list(\n",
    "    itertools.product(\n",
    "        WELLS,\n",
    "        SITES if isinstance(SITES, list) else range(1, 13) \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 3 images\n",
      "Validating on 1 images\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "random.shuffle(wells_and_sites)\n",
    "\n",
    "split = int(len(wells_and_sites) * 0.85)\n",
    "\n",
    "training_set = wells_and_sites[:split]\n",
    "validation_set = wells_and_sites[split:]\n",
    "\n",
    "print(\"Training on {0} images\".format(len(training_set)))\n",
    "print(\"Validating on {0} images\".format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 The root feature\n",
    "\n",
    "We use DeepTrack 2.0 to define the data loader pipeline. The pipeline is a sequence of `features`, which perform computations, controlled by `properties`, which are defined when creating the features. (Note that we any property with any name and value to a feature; if a property is not used by the feature, we refer to it as a dummy property.)\n",
    "\n",
    "The feature `root` is a `DummyFeature`, which is just a container of dummy properties and does not perform any computations.\n",
    "It takes the following arguments:\n",
    "\n",
    "* `well_site_tuple` is a dummy property that cycles through the well-site combinations in `wells_and_sites`\n",
    "* `well` is a dummy property that extracts the well from the `well_site_tuple`\n",
    "* `site` is a dummy property that extracts the site from the `well_site_tuple`\n",
    "\n",
    "Note that `well` and `site` are functions that take `well_site_tuple` as argument. These are dependent properties, and DeepTrack 2.0 will automatically ensure that they receive the correct input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = itertools.cycle(training_set)\n",
    "validation_iterator = itertools.cycle(validation_set)\n",
    "\n",
    "def get_next_well_and_site(validation):\n",
    "    if validation:\n",
    "        return next(validation_iterator)\n",
    "    else:\n",
    "        return next(training_iterator)\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the well\n",
    "def get_well_from_tuple(well_site_tuple):\n",
    "    return well_site_tuple[0]\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the site as \n",
    "# a string formated to be of length 3.\n",
    "def get_site_from_tuple(well_site_tuple):\n",
    "    site_string = \"00\" + str(well_site_tuple[1])\n",
    "    return site_string[-3:]\n",
    "\n",
    "\n",
    "\n",
    "root = dt.DummyFeature(\n",
    "    well_site_tuple=get_next_well_and_site,           # On each update, root will grab the next value from this iterator\n",
    "    well=get_well_from_tuple,                         # Grabs the well from the well_site_tuple\n",
    "    site=get_site_from_tuple,                         # Grabs and formats the site from the well_site_tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The brightfield image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a brightfield stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 7)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightfield_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, 4, z) for z in range(1, 8)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 The fluorescence image loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a fluorescence stack. It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we take the properties of `root` (of importance `well` and `site`). The other properties of LoadImage will now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a shaped volume with dimensions (width, height, 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, action, 1) for action in range(1, 4)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Offset adjustment\n",
    "\n",
    "Offset adjustments using affine transformations. The offset is parametrized as a function of the magnification and the site as described in the report.\n",
    "\n",
    "The properties are set as follows:\n",
    "* `translate` sets how much we translate the image in pixels. It is a tuple representing the (x, y) shift. We calculate it as a function of the angular position of the site within the well, with site 1 at angle 0.\n",
    "* `angle` is a dummy property that calculates the angle of the site in radians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients of the regression\n",
    "Ax = +2.3054\n",
    "Bx = -0.0315\n",
    "Ay = -0.1352\n",
    "By = -2.3049\n",
    "x =  -0.8363\n",
    "y =  +0.8081\n",
    "scale= 0.99975\n",
    "\n",
    "correct_offset = dt.Affine(\n",
    "    translate=lambda angle: (\n",
    "        (np.cos(angle) * Bx + np.sin(angle) * Ax + x) * -1, # Offset in x\n",
    "        (np.cos(angle) * By + np.sin(angle) * Ay + y) * -1, # Offset in y\n",
    "    ),\n",
    "    angle = lambda site: (int(site) - 1) * np.pi / 6,\n",
    "    **root.properties,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Define augmentations\n",
    "\n",
    "We use three kinds of augmentations: Mirroring (`deeptrack.FlipLR`), Affine transformations (`deeptrack.ElasticTransformation`), and Distortions (`deeptrack.Crop`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = dt.FlipLR()\n",
    "\n",
    "affine = dt.Affine(\n",
    "    rotate=lambda: np.random.rand() * 2 * np.pi,\n",
    "    scale=lambda: np.random.rand() * 0.1 + 0.95,\n",
    "    shear=lambda: np.random.rand() * 0.05 - 0.025\n",
    ")\n",
    "\n",
    "distortion = dt.ElasticTransformation(\n",
    "    alpha=lambda: np.random.rand() * 80,\n",
    "    sigma=lambda: 7\n",
    ")\n",
    "\n",
    "corner = 512 * (np.sqrt(2) - 1) / 2\n",
    "cropping  = dt.Crop(\n",
    "    crop=(512, 512, None),\n",
    "    corner=(corner, corner, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.7 Create the pipeline \n",
    "\n",
    "We use the (`+`) operator to chain the features, defining the execution order. In DeepTrack 2.0, this means that the output of the feature on the left, is passed as the input to the feature on the right. This is done in the following steps:\n",
    "\n",
    "1. `corrected_brightfield` is generated by offsetting the `brightfield_loader`\n",
    "2. `data_pair` is created with input images and targets\n",
    "3. `augmented_data` are defined by using the augmentations as well as the cropping\n",
    "4. `validation_data` is created\n",
    "5. `dataset` is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C02', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_brightfield = brightfield_loader + correct_offset\n",
    "\n",
    "data_pair = dt.Combine([corrected_brightfield, fluorescence_loader])\n",
    "\n",
    "padded_crop_size = int(512 * np.sqrt(2))\n",
    "\n",
    "cropped_data = dt.Crop(\n",
    "    data_pair,\n",
    "    crop=(padded_crop_size, padded_crop_size, None),\n",
    "    updates_per_reload=16,\n",
    "    corner=\"random\",\n",
    ")\n",
    "\n",
    "augmented_data = cropped_data + flip + affine + distortion + cropping\n",
    "\n",
    "validation_data = data_pair + dt.PadToMultiplesOf(multiple=(32, 32, None))\n",
    "\n",
    "dataset = dt.ConditionalSetFeature(\n",
    "    on_true=validation_data,\n",
    "    on_false=augmented_data,\n",
    "    condition=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define generator\n",
    "\n",
    "We use generators to interface DeepTrack 2.0 features with Keras training routines. In DeepTrack 2.0, we have defined some special generators that speed up training. Here, we will use `deeptrack.ContinuousGenerator`, which continuosly geenrate augmented training images and makes them available for training the neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = dt.generators.ContinuousGenerator(\n",
    "    dataset,\n",
    "    batch_function=lambda image: image[0],\n",
    "    label_function=lambda image: image[1],\n",
    "    batch_size=8,\n",
    "    min_data_size=500,\n",
    "    max_data_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define model\n",
    "\n",
    "Here, we use a GAN with a U-Net generator and a convolutional encoder discriminator. This uses MAE loss in the generator and MSE in the discriminator. The details are described in the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_generator = apido.generator(GENERATOR_BREADTH, GENERATOR_WIDTH)\n",
    "GAN_discriminator = apido.discriminator(DISCRIMINATOR_DEPTH)\n",
    "\n",
    "GAN = dt.models.cgan(\n",
    "    generator=GAN_generator,\n",
    "    discriminator=GAN_discriminator,\n",
    "    discriminator_loss=\"mse\",\n",
    "    assemble_loss=[\"mse\", \"mae\"],\n",
    "    assemble_loss_weights=[1 - MAE_LOSS_WEIGHT, MAE_LOSS_WEIGHT]\n",
    ")\n",
    "\n",
    "GAN.compile(metrics=apido.metrics(\"60x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train model\n",
    "\n",
    "We finally train the model for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 / 500 samples before starting training\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\bmidt\\appdata\\local\\programs\\python\\python37\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\bmidt\\appdata\\local\\programs\\python\\python37\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\", line 329, in _continuous_get_training_data\n",
      "    new_image = self._get(self.feature, self.feature_kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\", line 370, in _get\n",
      "    return features.resolve(**feature_kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 725, in get\n",
      "    return on_false.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\augmentations.py\", line 166, in _process_and_get\n",
      "    output = self.get(image_list, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\augmentations.py\", line 651, in get\n",
      "    crop_amount = np.array(image.shape) - np.array(crop)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 / 500 samples before starting training\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9dbf5c8a13a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     h = GAN.fit(\n\u001b[0;32m      5\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     )\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             print(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_data = zip(dataset.update(validation=True).resolve() for _ in range(len(validation_set)))\n",
    "\n",
    "with generator:\n",
    "    h = GAN.fit(\n",
    "        generator, \n",
    "        epochs=500,\n",
    "        steps_per_epoch=32,\n",
    "        validation_data=validation_data,\n",
    "        validation_batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
