{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train 60x Virtual Stainer\n",
    "\n",
    "Notebook by Group 1: Team-Soft-Matter-Lab-GU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Globals\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import itertools\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "# Locals\n",
    "import apido\n",
    "import deeptrack as dt\n",
    "\n",
    "# Packages\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Constants\n",
    "\n",
    "Sets of constants used by the notebook\n",
    "\n",
    "### 1.1 User constants\n",
    "\n",
    "Constants set by the user\n",
    "\n",
    "* `DATASET_PATH`: Input path (not including the magnification folder)\n",
    "\n",
    "* `OUTPUT_PATH`: Output path (not including the magnication folder)\n",
    "\n",
    "* `WELLS`: Name of the wells to predict on\n",
    "\n",
    "* `SITES`: \"all\" or list of integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATOR_BREADTH = 16\n",
    "GENERATOR_DEPTH = 5\n",
    "DISCRIMINATOR_DEPTH = 5\n",
    "MAE_LOSS_WEIGHT = 0.001\n",
    "\n",
    "DATASET_PATH = \"./local_data/\" \n",
    "OUTPUT_PATH = \"./models/\"\n",
    "WELLS = [\"C02\"]\n",
    "SITES = [1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Inferred constants\n",
    "\n",
    "Constants inferred from the user input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGNIFICATION = \"60x\"\n",
    "file_name_struct = \"AssayPlate_Greiner_#655090_{0}_T0001F{1}L01A0{2}Z0{3}C0{2}.tif\"\n",
    "\n",
    "PATH_TO_OUTPUT = os.path.normpath(OUTPUT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Infer full path to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_glob_struct = os.path.join(DATASET_PATH, MAGNIFICATION + \"*/\")\n",
    "_glob_results = glob.glob(_glob_struct)\n",
    "\n",
    "if len(_glob_results) == 0:\n",
    "    raise ValueError(\"No path found matching glob {0}\".format(_glob_struct))\n",
    "elif len(_glob_results) > 1:\n",
    "    from warnings import warn\n",
    "    warn(\"Multiple paths found! Using {0}\".format(_glob_results[0]))\n",
    "\n",
    "PATH_TO_MAGNIFICATION = os.path.normpath(_glob_results[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading images from: \t local_data\\60x images\n",
      "Saving results to: \t models\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading images from: \\t\", PATH_TO_MAGNIFICATION)\n",
    "print(\"Saving results to: \\t\", PATH_TO_OUTPUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load model\n",
    "\n",
    "We load the virtual stainer from the local path.\n",
    "This is expected to warn about overwriting `groups`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data loader\n",
    "\n",
    "We define a data pipeline for loading images from storage. This uses DeepTrack 2.0, and follows the structure of\n",
    "\n",
    "1. Load each z-slice of an well-site combination and concatenate them.\n",
    "2. Pad the volume such that the first two dimensions are multiples of 32 (required by the model).\n",
    "3. Correct for misalignment of the fluorescence channel and the brightfield channel (by a pre-calculated parametrization of the offset as a function of magnification and the site.)\n",
    "\n",
    "\n",
    "### 3.1 Find all wells and sites\n",
    "\n",
    "We create an iterator over each well and site. `Itertools.product` produces an iterator over each combination of its input. In this case, each site in each well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wells_and_sites = list(\n",
    "    itertools.product(\n",
    "        WELLS,\n",
    "        SITES if isinstance(SITES, list) else range(1, 13) \n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 3 images\n",
      "Validating on 1 images\n"
     ]
    }
   ],
   "source": [
    "random.seed(1)\n",
    "random.shuffle(wells_and_sites)\n",
    "\n",
    "split = int(len(wells_and_sites) * 0.85)\n",
    "\n",
    "training_set = wells_and_sites[:split]\n",
    "validation_set = wells_and_sites[split:]\n",
    "\n",
    "print(\"Training on {0} images\".format(len(training_set)))\n",
    "print(\"Validating on {0} images\".format(len(validation_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 The root feature\n",
    "\n",
    "We use DeepTrack 2 to define the data loader pipeline. The pipeline is a sequence of `features`, which perform computations. They are controlled by `properties`, which we pass when we create the feature.\n",
    "\n",
    "As an example, `root` is a `DummyFeature`, which does not perform any computations, and is instead just a container of properties. Improtant to note, we can pass any argument of any name to the `feature`. If it is not used by the `feature`, we refer to it as a dummy property.\n",
    "\n",
    "It takes the following arguments:\n",
    "\n",
    "* `well_site_tuple` is a dummy property that cycles through the well-site combinations in `wells_and_sites`\n",
    "* `well` is a dummy property that extracts the well from the `well_site_tuple`\n",
    "* `site` is a dummy property that extracts the site from the `well_site_tuple`\n",
    "\n",
    "Note that `well` and `site` are functions that take `well_site_tuple` as argument. These are dependent properties, and deeptrack will automatically ensure that they receive the correct input.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iterator = itertools.cycle(training_set)\n",
    "validation_iterator = itertools.cycle(validation_set)\n",
    "\n",
    "def get_next_well_and_site(validation):\n",
    "    if validation:\n",
    "        return next(validation_iterator)\n",
    "    else:\n",
    "        return next(training_iterator)\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the well\n",
    "def get_well_from_tuple(well_site_tuple):\n",
    "    return well_site_tuple[0]\n",
    "\n",
    "# Accepts a tuple of form (well, site), and returns the site as \n",
    "# a string formated to be of length 3.\n",
    "def get_site_from_tuple(well_site_tuple):\n",
    "    site_string = \"00\" + str(well_site_tuple[1])\n",
    "    return site_string[-3:]\n",
    "\n",
    "\n",
    "\n",
    "root = dt.DummyFeature(\n",
    "    well_site_tuple=get_next_well_and_site,           # On each update, root will grab the next value from this iterator\n",
    "    well=get_well_from_tuple,                         # Grabs the well from the well_site_tuple\n",
    "    site=get_site_from_tuple,                         # Grabs and formats the site from the well_site_tuple\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 The brightfield loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a brightfield stack.\n",
    "\n",
    "It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we grab the properties of `root` (of importance `well` and `site`). Other properties of LoadImage can now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a (width, height, 7) shaped volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "brightfield_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, 4, z) for z in range(1, 8)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 The fluorescence loader\n",
    "\n",
    "We use `deeptrack.LoadImage` to load and concatenate a fluorescence stack.\n",
    "\n",
    "It takes the following arguments:\n",
    "\n",
    "* `**root.properties` means that we grab the properties of `root` (of importance `well` and `site`). Other properties of LoadImage can now depend on these.\n",
    "* `file_names` is a dummy property, which takes the current well and site as input, and creates a list of file names that we want to load.\n",
    "* `path` is a property used by `LoadImage` to determine which files to load. We calculate it by taking `file_names` as input and returning a list of paths using `os.path.join`.\n",
    "\n",
    "Since `path` is a list, `LoadImage` stacks the images along the last dimension, creating a (width, height, 3) shaped volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluorescence_loader = dt.LoadImage(\n",
    "    **root.properties,\n",
    "    file_names=lambda well, site: [file_name_struct.format(well, site, action, 1) for action in range(1, 4)],\n",
    "    path=lambda file_names: [os.path.join(PATH_TO_MAGNIFICATION, file_name) for file_name in file_names],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Offset adjustment\n",
    "\n",
    "Offset adjustments using affine transformations. The offset is parametrized as a function of the magnification and the site.\n",
    "\n",
    "The properties are set as follows:\n",
    "* `translate` sets how much we translate the image in pixels. It is a tuple representing the (x, y) shift. We calculate it as a function of angle of the site, with site 1 at angle 0.\n",
    "* `angle` is a dummy property that calculates the angle of the site in radians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Coefficients of the regression\n",
    "Ax = +2.3054\n",
    "Bx = -0.0315\n",
    "Ay = -0.1352\n",
    "By = -2.3049\n",
    "x =  -0.8363\n",
    "y =  +0.8081\n",
    "scale= 0.99975\n",
    "\n",
    "correct_offset = dt.Affine(\n",
    "    translate=lambda angle: (\n",
    "        (np.cos(angle) * Bx + np.sin(angle) * Ax + x) * -1, # Offset in x\n",
    "        (np.cos(angle) * By + np.sin(angle) * Ay + y) * -1, # Offset in y\n",
    "    ),\n",
    "    angle = lambda site: (int(site) - 1) * np.pi / 6,\n",
    "    **root.properties,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Augmentations\n",
    "\n",
    "We use three kinds of augmentations: Mirroring, Affine transformations, Distortions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "flip = dt.FlipLR()\n",
    "\n",
    "affine = dt.Affine(\n",
    "    rotate=lambda: np.random.rand() * 2 * np.pi,\n",
    "    scale=lambda: np.random.rand() * 0.1 + 0.95,\n",
    "    shear=lambda: np.random.rand() * 0.05 - 0.025\n",
    ")\n",
    "\n",
    "distortion = dt.ElasticTransformation(\n",
    "    alpha=lambda: np.random.rand() * 80,\n",
    "    sigma=lambda: 7\n",
    ")\n",
    "\n",
    "corner = 512 * (np.sqrt(2) - 1) / 2\n",
    "cropping  = dt.Crop(\n",
    "    crop=(512, 512, None),\n",
    "    corner=(corner, corner, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Creating the pipeline \n",
    "\n",
    "We use the (`+`) operator to chain the features, defining the execution order.\n",
    "\n",
    "In DeepTrack, this means that the output of the feature on the left, is passed as the input to the feature on the right.\n",
    "\n",
    "In other words, the stack loaded by `brightfield_loader` is passed to `ensure_padded`, the output of which is offset-corrected by `correct_offset`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('C02', 2)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corrected_brightfield = brightfield_loader + correct_offset\n",
    "\n",
    "data_pair = dt.Combine([corrected_brightfield, fluorescence_loader])\n",
    "\n",
    "validation_set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_crop_size = int(512 * np.sqrt(2))\n",
    "\n",
    "cropped_data = dt.Crop(\n",
    "    data_pair,\n",
    "    crop=(padded_crop_size, padded_crop_size, None),\n",
    "    updates_per_reload=16,\n",
    "    corner=\"random\",\n",
    ")\n",
    "\n",
    "augmented_data = cropped_data + flip + affine + distortion + cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data_pair + dt.PadToMultiplesOf(multiple=(32, 32, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dt.ConditionalSetFeature(\n",
    "    on_true=validation_data,\n",
    "    on_false=augmented_data,\n",
    "    condition=\"validation\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define generator\n",
    "\n",
    "We use generators to interface DeepTrack features with keras training. We have defined some special ones, which speed up training.\n",
    "\n",
    "Here we will use `ContinuousGenerator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = dt.generators.ContinuousGenerator(\n",
    "    dataset,\n",
    "    batch_function=lambda image: image[0],\n",
    "    label_function=lambda image: image[1],\n",
    "    batch_size=8,\n",
    "    min_data_size=500,\n",
    "    max_data_size=2000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAN_generator = apido.generator(GENERATOR_BREADTH, GENERATOR_WIDTH)\n",
    "GAN_discriminator = apido.discriminator(DISCRIMINATOR_DEPTH)\n",
    "\n",
    "GAN = dt.models.cgan(\n",
    "    generator=GAN_generator,\n",
    "    discriminator=GAN_discriminator,\n",
    "    discriminator_loss=\"mse\",\n",
    "    assemble_loss=[\"mse\", \"mae\"],\n",
    "    assemble_loss_weights=[1 - MAE_LOSS_WEIGHT, MAE_LOSS_WEIGHT]\n",
    ")\n",
    "\n",
    "GAN.compile(metrics=apido.metrics(\"60x\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 / 500 samples before starting training\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\users\\bmidt\\appdata\\local\\programs\\python\\python37\\lib\\threading.py\", line 926, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\users\\bmidt\\appdata\\local\\programs\\python\\python37\\lib\\threading.py\", line 870, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\", line 329, in _continuous_get_training_data\n",
      "    new_image = self._get(self.feature, self.feature_kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\", line 370, in _get\n",
      "    return features.resolve(**feature_kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 725, in get\n",
      "    return on_false.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 357, in _process_and_get\n",
      "    new_list = self.get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 473, in get\n",
      "    image = feature_1.resolve(image, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\features.py\", line 193, in resolve\n",
      "    new_list = self._process_and_get(image_list, **feature_input)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\augmentations.py\", line 166, in _process_and_get\n",
      "    output = self.get(image_list, **kwargs)\n",
      "  File \"D:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\augmentations.py\", line 651, in get\n",
      "    crop_amount = np.array(image.shape) - np.array(crop)\n",
      "TypeError: unsupported operand type(s) for -: 'int' and 'NoneType'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 0 / 500 samples before starting training\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-9dbf5c8a13a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresolve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     h = GAN.fit(\n\u001b[0;32m      5\u001b[0m         \u001b[0mgenerator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    278\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKeyboardInterrupt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 280\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    281\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Team-Soft-Matter-Lab-GU\\src\\deeptrack\\generators.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    267\u001b[0m                         \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m                     )\n\u001b[1;32m--> 269\u001b[1;33m                 \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    270\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    271\u001b[0m             print(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "validation_data = zip(dataset.update(validation=True).resolve() for _ in range(len(validation_set)))\n",
    "\n",
    "with generator:\n",
    "    h = GAN.fit(\n",
    "        generator, \n",
    "        epochs=500,\n",
    "        steps_per_epoch=32,\n",
    "        validation_data=validation_data,\n",
    "        validation_batch_size=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save results\n",
    "\n",
    "Iterate over the stains, and store the results to memory. In DeepTrack, the properties used to create a image using `resolve()` are stored in a field called `properties`, and can easily be retrieved using the utility method `get_property`.\n",
    "\n",
    "Here, we use this to extract the well and site of the image, which is needed to correctly name the file. Moreover, some features save additional values. One such case is `undo_padding` which is saved by all padding features. This is a tuple of slices that return an numpy array to its pre-padded size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apido.save_training_results(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
